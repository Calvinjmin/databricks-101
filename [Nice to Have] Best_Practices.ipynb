{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Data Engineering: Best Practices & Advanced Patterns\n",
        "\n",
        "**Production-Ready Patterns for Data Engineering on Databricks**\n",
        "\n",
        "This notebook covers advanced topics and best practices for building robust, scalable data pipelines.\n",
        "\n",
        "## Topics Covered:\n",
        "\n",
        "- üîÑ **Incremental Processing**: Handle updates efficiently\n",
        "- üîç **Data Quality**: Validation frameworks and error handling\n",
        "- ‚ö° **Performance Optimization**: Partitioning, caching, and Z-ordering\n",
        "- üîí **Idempotency**: Make pipelines safe to re-run\n",
        "- üìä **Monitoring & Logging**: Track pipeline health\n",
        "- üéØ **Advanced Delta Lake**: Merge operations, SCD patterns\n",
        "- üß™ **Testing**: Data quality tests and pipeline validation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Incremental Data Processing\n",
        "\n",
        "Instead of reprocessing all data, process only new or changed records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "print(\"‚úÖ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 1: Watermark-Based Incremental Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track the last processed timestamp\n",
        "def get_last_watermark(table_name):\n",
        "    \"\"\"Get the max timestamp from the last successful load\"\"\"\n",
        "    try:\n",
        "        max_timestamp = spark.sql(f\"SELECT MAX(updated_at) as max_ts FROM {table_name}\").collect()[0][0]\n",
        "        return max_timestamp if max_timestamp else \"1900-01-01\"\n",
        "    except:\n",
        "        return \"1900-01-01\"\n",
        "\n",
        "# Example: Only process new orders\n",
        "# last_processed = get_last_watermark(\"silver.orders\")\n",
        "# new_orders = spark.sql(f\"\"\"\n",
        "#     SELECT * FROM bronze.orders\n",
        "#     WHERE _ingestion_timestamp > '{last_processed}'\n",
        "# \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 2: Merge (Upsert) Operations\n",
        "\n",
        "Handle inserts and updates in a single operation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Upsert customer data\n",
        "# Assume we have updated customer data in a DataFrame called `updated_customers`\n",
        "\n",
        "# from delta.tables import DeltaTable\n",
        "# \n",
        "# target_table = DeltaTable.forName(spark, \"silver.customers\")\n",
        "# \n",
        "# target_table.alias(\"target\").merge(\n",
        "#     updated_customers.alias(\"source\"),\n",
        "#     \"target.customer_id = source.customer_id\"\n",
        "# ).whenMatchedUpdate(\n",
        "#     set = {\n",
        "#         \"first_name\": \"source.first_name\",\n",
        "#         \"last_name\": \"source.last_name\",\n",
        "#         \"email\": \"source.email\",\n",
        "#         \"updated_at\": \"current_timestamp()\"\n",
        "#     }\n",
        "# ).whenNotMatchedInsert(\n",
        "#     values = {\n",
        "#         \"customer_id\": \"source.customer_id\",\n",
        "#         \"first_name\": \"source.first_name\",\n",
        "#         \"last_name\": \"source.last_name\",\n",
        "#         \"email\": \"source.email\",\n",
        "#         \"updated_at\": \"current_timestamp()\"\n",
        "#     }\n",
        "# ).execute()\n",
        "\n",
        "print(\"‚úÖ Merge pattern example\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Data Quality Framework\n",
        "\n",
        "Build robust data quality checks into your pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Check Framework\n",
        "class DataQualityValidator:\n",
        "    \"\"\"Simple data quality validation framework\"\"\"\n",
        "    \n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.checks = []\n",
        "        \n",
        "    def check_not_null(self, column):\n",
        "        \"\"\"Ensure column has no nulls\"\"\"\n",
        "        null_count = self.df.filter(col(column).isNull()).count()\n",
        "        self.checks.append({\n",
        "            \"check\": f\"{column} NOT NULL\",\n",
        "            \"passed\": null_count == 0,\n",
        "            \"failed_records\": null_count\n",
        "        })\n",
        "        return self\n",
        "    \n",
        "    def check_range(self, column, min_val, max_val):\n",
        "        \"\"\"Ensure numeric column is within range\"\"\"\n",
        "        out_of_range = self.df.filter(\n",
        "            (col(column) < min_val) | (col(column) > max_val)\n",
        "        ).count()\n",
        "        self.checks.append({\n",
        "            \"check\": f\"{column} BETWEEN {min_val} AND {max_val}\",\n",
        "            \"passed\": out_of_range == 0,\n",
        "            \"failed_records\": out_of_range\n",
        "        })\n",
        "        return self\n",
        "    \n",
        "    def check_unique(self, column):\n",
        "        \"\"\"Ensure column values are unique\"\"\"\n",
        "        total_count = self.df.count()\n",
        "        unique_count = self.df.select(column).distinct().count()\n",
        "        self.checks.append({\n",
        "            \"check\": f\"{column} UNIQUE\",\n",
        "            \"passed\": total_count == unique_count,\n",
        "            \"failed_records\": total_count - unique_count\n",
        "        })\n",
        "        return self\n",
        "    \n",
        "    def report(self):\n",
        "        \"\"\"Print validation report\"\"\"\n",
        "        print(\"=\"*70)\n",
        "        print(\"DATA QUALITY VALIDATION REPORT\")\n",
        "        print(\"=\"*70)\n",
        "        for check in self.checks:\n",
        "            status = \"‚úÖ PASS\" if check[\"passed\"] else \"‚ùå FAIL\"\n",
        "            print(f\"{status} | {check['check']}\")\n",
        "            if not check[\"passed\"]:\n",
        "                print(f\"     Failed records: {check['failed_records']}\")\n",
        "        print(\"=\"*70)\n",
        "        return all(c[\"passed\"] for c in self.checks)\n",
        "\n",
        "# Example usage:\n",
        "# validator = DataQualityValidator(spark.table(\"silver.orders\"))\n",
        "# validator.check_not_null(\"order_id\") \\\n",
        "#          .check_not_null(\"customer_id\") \\\n",
        "#          .check_range(\"discount_percent\", 0, 100) \\\n",
        "#          .check_unique(\"order_id\") \\\n",
        "#          .report()\n",
        "\n",
        "print(\"‚úÖ Data quality framework defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partitioning\n",
        "\n",
        "Partition large tables by commonly filtered columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Example: Partition orders by year and month\n",
        "-- CREATE TABLE silver.orders_partitioned (\n",
        "--   order_id BIGINT,\n",
        "--   customer_id BIGINT,\n",
        "--   order_date TIMESTAMP,\n",
        "--   status STRING,\n",
        "--   -- other columns...\n",
        "-- ) USING DELTA\n",
        "-- PARTITIONED BY (order_year, order_month);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Z-Ordering\n",
        "\n",
        "Optimize for queries that filter on specific columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Z-order by columns frequently used in WHERE clauses\n",
        "-- OPTIMIZE silver.orders ZORDER BY (customer_id, status);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching\n",
        "\n",
        "Cache frequently accessed DataFrames:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache tables you'll query multiple times\n",
        "# customers_df = spark.table(\"silver.customers\").cache()\n",
        "# products_df = spark.table(\"silver.products\").cache()\n",
        "\n",
        "# Don't forget to unpersist when done\n",
        "# customers_df.unpersist()\n",
        "\n",
        "print(\"‚úÖ Caching examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Slowly Changing Dimensions (SCD Type 2)\n",
        "\n",
        "Track historical changes to dimension tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Customer SCD Type 2 implementation\n",
        "# \n",
        "# SCD Type 2 maintains full history by:\n",
        "# - Adding effective_date and end_date columns\n",
        "# - Adding is_current flag\n",
        "# - Closing old records and inserting new ones on change\n",
        "\n",
        "# Schema for SCD Type 2:\n",
        "# - customer_id\n",
        "# - first_name, last_name, email, etc.\n",
        "# - effective_date (when this version became active)\n",
        "# - end_date (when this version was superseded, NULL if current)\n",
        "# - is_current (TRUE if this is the current version)\n",
        "\n",
        "print(\"\"\"\n",
        "Example SCD Type 2 Logic:\n",
        "\n",
        "1. New records: Insert with is_current=True, end_date=NULL\n",
        "2. Changed records: \n",
        "   - Update old record: Set is_current=False, end_date=today\n",
        "   - Insert new record: Set is_current=True, end_date=NULL, effective_date=today\n",
        "3. Unchanged records: No action needed\n",
        "\n",
        "This allows you to query:\n",
        "- Current state: WHERE is_current = True\n",
        "- Historical state: WHERE effective_date <= '2023-01-01' AND (end_date > '2023-01-01' OR end_date IS NULL)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Error Handling & Dead Letter Queue\n",
        "\n",
        "Gracefully handle bad records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern: Separate good and bad records\n",
        "\n",
        "# # Example: Validate and split records\n",
        "# all_records = spark.table(\"bronze.orders\")\n",
        "# \n",
        "# # Good records pass validation\n",
        "# good_records = all_records.filter(\n",
        "#     (col(\"order_id\").isNotNull()) &\n",
        "#     (col(\"customer_id\").isNotNull()) &\n",
        "#     (col(\"order_date\").isNotNull())\n",
        "# )\n",
        "# \n",
        "# # Bad records fail validation - send to quarantine table\n",
        "# bad_records = all_records.exceptAll(good_records) \\\n",
        "#     .withColumn(\"rejection_reason\", lit(\"Missing required fields\")) \\\n",
        "#     .withColumn(\"rejected_at\", current_timestamp())\n",
        "# \n",
        "# # Write bad records to dead letter queue for investigation\n",
        "# bad_records.write.mode(\"append\").saveAsTable(\"quarantine.orders_dlq\")\n",
        "# \n",
        "# # Process only good records\n",
        "# good_records.write.mode(\"append\").saveAsTable(\"silver.orders\")\n",
        "\n",
        "print(\"‚úÖ Error handling pattern\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Monitoring & Observability\n",
        "\n",
        "Track pipeline metrics for production monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log pipeline metrics to a monitoring table\n",
        "\n",
        "# from datetime import datetime\n",
        "# \n",
        "# def log_pipeline_metrics(pipeline_name, status, records_processed, error_message=None):\n",
        "#     \"\"\"Log pipeline execution metrics\"\"\"\n",
        "#     metrics = [{\n",
        "#         \"pipeline_name\": pipeline_name,\n",
        "#         \"execution_time\": datetime.now(),\n",
        "#         \"status\": status,  # 'SUCCESS', 'FAILED', 'PARTIAL'\n",
        "#         \"records_processed\": records_processed,\n",
        "#         \"error_message\": error_message,\n",
        "#         \"spark_app_id\": spark.sparkContext.applicationId\n",
        "#     }]\n",
        "#     \n",
        "#     spark.createDataFrame(metrics).write.mode(\"append\").saveAsTable(\"monitoring.pipeline_runs\")\n",
        "# \n",
        "# # Usage:\n",
        "# try:\n",
        "#     records_count = process_orders()\n",
        "#     log_pipeline_metrics(\"orders_pipeline\", \"SUCCESS\", records_count)\n",
        "# except Exception as e:\n",
        "#     log_pipeline_metrics(\"orders_pipeline\", \"FAILED\", 0, str(e))\n",
        "#     raise\n",
        "\n",
        "print(\"‚úÖ Monitoring pattern\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Key Takeaways\n",
        "\n",
        "### Production-Ready Checklist:\n",
        "\n",
        "- ‚úÖ **Idempotent**: Can safely re-run without duplicating data\n",
        "- ‚úÖ **Incremental**: Only processes new/changed data\n",
        "- ‚úÖ **Validated**: Data quality checks at each layer\n",
        "- ‚úÖ **Monitored**: Logs metrics for observability\n",
        "- ‚úÖ **Resilient**: Handles errors gracefully with DLQ\n",
        "- ‚úÖ **Performant**: Optimized with partitioning and Z-ordering\n",
        "- ‚úÖ **Documented**: Clear lineage and metadata\n",
        "- ‚úÖ **Tested**: Unit and integration tests\n",
        "\n",
        "### Common Anti-Patterns to Avoid:\n",
        "\n",
        "- ‚ùå Full table scans on every run\n",
        "- ‚ùå No data quality checks\n",
        "- ‚ùå Silent failures (no error handling)\n",
        "- ‚ùå Hardcoded values instead of parameters\n",
        "- ‚ùå No monitoring or alerting\n",
        "- ‚ùå Overwriting data without history\n",
        "- ‚ùå Missing indexes on large tables\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)\n",
        "- [Databricks Performance Tuning](https://docs.databricks.com/optimizations/index.html)\n",
        "- [Data Quality on Databricks](https://www.databricks.com/blog/2022/01/19/data-quality-at-scale-with-databricks.html)\n",
        "\n",
        "---\n",
        "\n",
        "**Continue learning and building robust data pipelines! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
