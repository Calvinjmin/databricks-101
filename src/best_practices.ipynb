{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Data Engineering: Best Practices & Advanced Patterns\n\n**Production-Ready Patterns for Data Engineering on Databricks**\n\nThis notebook covers advanced topics and best practices for building robust, scalable data pipelines.\n\n## Topics Covered:\n\n- \ud83d\udd04 **Incremental Processing**: Handle updates efficiently\n- \ud83d\udd0d **Data Quality**: Validation frameworks and error handling\n- \u26a1 **Performance Optimization**: Partitioning, caching, and Z-ordering\n- \ud83d\udd12 **Idempotency**: Make pipelines safe to re-run\n- \ud83d\udcca **Monitoring & Logging**: Track pipeline health\n- \ud83c\udfaf **Advanced Delta Lake**: Merge operations, SCD patterns\n- \ud83e\uddea **Testing**: Data quality tests and pipeline validation\n\n---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Incremental Data Processing\n\nInstead of reprocessing all data, process only new or changed records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom delta.tables import DeltaTable\n\nprint(\"\u2705 Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 1: Watermark-Based Incremental Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track the last processed timestamp\ndef get_last_watermark(table_name):\n    \"\"\"Get the max timestamp from the last successful load\"\"\"\n    try:\n        max_timestamp = spark.sql(f\"SELECT MAX(updated_at) as max_ts FROM {table_name}\").collect()[0][0]\n        return max_timestamp if max_timestamp else \"1900-01-01\"\n    except:\n        return \"1900-01-01\"\n\n# Example: Only process new orders\n# last_processed = get_last_watermark(\"demo.silver.orders\")\n# new_orders = spark.sql(f\"\"\"\n#     SELECT * FROM demo.bronze.orders\n#     WHERE _ingestion_timestamp > '{last_processed}'\n# \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pattern 2: Merge (Upsert) Operations\n\nHandle inserts and updates in a single operation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Upsert customer data\n# Assume we have updated customer data in a DataFrame called `updated_customers`\n\n# from delta.tables import DeltaTable\n# \n# target_table = DeltaTable.forName(spark, \"demo.silver.customers\")\n# \n# target_table.alias(\"target\").merge(\n#     updated_customers.alias(\"source\"),\n#     \"target.customer_id = source.customer_id\"\n# ).whenMatchedUpdate(\n#     set = {\n#         \"first_name\": \"source.first_name\",\n#         \"last_name\": \"source.last_name\",\n#         \"email\": \"source.email\",\n#         \"updated_at\": \"current_timestamp()\"\n#     }\n# ).whenNotMatchedInsert(\n#     values = {\n#         \"customer_id\": \"source.customer_id\",\n#         \"first_name\": \"source.first_name\",\n#         \"last_name\": \"source.last_name\",\n#         \"email\": \"source.email\",\n#         \"updated_at\": \"current_timestamp()\"\n#     }\n# ).execute()\n\nprint(\"\u2705 Merge pattern example\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 2. Data Quality Framework\n\nBuild robust data quality checks into your pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Check Framework\nclass DataQualityValidator:\n    \"\"\"Simple data quality validation framework\"\"\"\n    \n    def __init__(self, df):\n        self.df = df\n        self.checks = []\n        \n    def check_not_null(self, column):\n        \"\"\"Ensure column has no nulls\"\"\"\n        null_count = self.df.filter(col(column).isNull()).count()\n        self.checks.append({\n            \"check\": f\"{column} NOT NULL\",\n            \"passed\": null_count == 0,\n            \"failed_records\": null_count\n        })\n        return self\n    \n    def check_range(self, column, min_val, max_val):\n        \"\"\"Ensure numeric column is within range\"\"\"\n        out_of_range = self.df.filter(\n            (col(column) < min_val) | (col(column) > max_val)\n        ).count()\n        self.checks.append({\n            \"check\": f\"{column} BETWEEN {min_val} AND {max_val}\",\n            \"passed\": out_of_range == 0,\n            \"failed_records\": out_of_range\n        })\n        return self\n    \n    def check_unique(self, column):\n        \"\"\"Ensure column values are unique\"\"\"\n        total_count = self.df.count()\n        unique_count = self.df.select(column).distinct().count()\n        self.checks.append({\n            \"check\": f\"{column} UNIQUE\",\n            \"passed\": total_count == unique_count,\n            \"failed_records\": total_count - unique_count\n        })\n        return self\n    \n    def report(self):\n        \"\"\"Print validation report\"\"\"\n        print(\"=\"*70)\n        print(\"DATA QUALITY VALIDATION REPORT\")\n        print(\"=\"*70)\n        for check in self.checks:\n            status = \"\u2705 PASS\" if check[\"passed\"] else \"\u274c FAIL\"\n            print(f\"{status} | {check['check']}\")\n            if not check[\"passed\"]:\n                print(f\"     Failed records: {check['failed_records']}\")\n        print(\"=\"*70)\n        return all(c[\"passed\"] for c in self.checks)\n\n# Example usage:\n# validator = DataQualityValidator(spark.table(\"demo.silver.orders\"))\n# validator.check_not_null(\"order_id\") \\\n#          .check_not_null(\"customer_id\") \\\n#          .check_range(\"discount_percent\", 0, 100) \\\n#          .check_unique(\"order_id\") \\\n#          .report()\n\nprint(\"\u2705 Data quality framework defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partitioning\n\nPartition large tables by commonly filtered columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Example: Partition orders by year and month\n-- CREATE TABLE demo.silver.orders_partitioned (\n--   order_id BIGINT,\n--   customer_id BIGINT,\n--   order_date TIMESTAMP,\n--   status STRING,\n--   -- other columns...\n-- ) USING DELTA\n-- PARTITIONED BY (order_year, order_month);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Z-Ordering\n\nOptimize for queries that filter on specific columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Z-order by columns frequently used in WHERE clauses\n-- OPTIMIZE silver.orders ZORDER BY (customer_id, status);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caching\n\nCache frequently accessed DataFrames:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache tables you'll query multiple times\n# customers_df = spark.table(\"demo.silver.customers\").cache()\n# products_df = spark.table(\"demo.silver.products\").cache()\n\n# Don't forget to unpersist when done\n# customers_df.unpersist()\n\nprint(\"\u2705 Caching examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 4. Slowly Changing Dimensions (SCD Type 2)\n\nTrack historical changes to dimension tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Customer SCD Type 2 implementation\n# \n# SCD Type 2 maintains full history by:\n# - Adding effective_date and end_date columns\n# - Adding is_current flag\n# - Closing old records and inserting new ones on change\n\n# Schema for SCD Type 2:\n# - customer_id\n# - first_name, last_name, email, etc.\n# - effective_date (when this version became active)\n# - end_date (when this version was superseded, NULL if current)\n# - is_current (TRUE if this is the current version)\n\nprint(\"\"\"\nExample SCD Type 2 Logic:\n\n1. New records: Insert with is_current=True, end_date=NULL\n2. Changed records: \n   - Update old record: Set is_current=False, end_date=today\n   - Insert new record: Set is_current=True, end_date=NULL, effective_date=today\n3. Unchanged records: No action needed\n\nThis allows you to query:\n- Current state: WHERE is_current = True\n- Historical state: WHERE effective_date <= '2023-01-01' AND (end_date > '2023-01-01' OR end_date IS NULL)\n\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 5. Error Handling & Dead Letter Queue\n\nGracefully handle bad records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pattern: Separate good and bad records\n\n# # Example: Validate and split records\n# all_records = spark.table(\"demo.bronze.orders\")\n# \n# # Good records pass validation\n# good_records = all_records.filter(\n#     (col(\"order_id\").isNotNull()) &\n#     (col(\"customer_id\").isNotNull()) &\n#     (col(\"order_date\").isNotNull())\n# )\n# \n# # Bad records fail validation - send to quarantine table\n# bad_records = all_records.exceptAll(good_records) \\\n#     .withColumn(\"rejection_reason\", lit(\"Missing required fields\")) \\\n#     .withColumn(\"rejected_at\", current_timestamp())\n# \n# # Write bad records to dead letter queue for investigation\n# bad_records.write.mode(\"append\").saveAsTable(\"quarantine.orders_dlq\")\n# \n# # Process only good records\n# good_records.write.mode(\"append\").saveAsTable(\"demo.silver.orders\")\n\nprint(\"\u2705 Error handling pattern\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 6. Monitoring & Observability\n\nTrack pipeline metrics for production monitoring.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log pipeline metrics to a monitoring table\n\n# from datetime import datetime\n# \n# def log_pipeline_metrics(pipeline_name, status, records_processed, error_message=None):\n#     \"\"\"Log pipeline execution metrics\"\"\"\n#     metrics = [{\n#         \"pipeline_name\": pipeline_name,\n#         \"execution_time\": datetime.now(),\n#         \"status\": status,  # 'SUCCESS', 'FAILED', 'PARTIAL'\n#         \"records_processed\": records_processed,\n#         \"error_message\": error_message,\n#         \"spark_app_id\": spark.sparkContext.applicationId\n#     }]\n#     \n#     spark.createDataFrame(metrics).write.mode(\"append\").saveAsTable(\"monitoring.pipeline_runs\")\n# \n# # Usage:\n# try:\n#     records_count = process_orders()\n#     log_pipeline_metrics(\"orders_pipeline\", \"SUCCESS\", records_count)\n# except Exception as e:\n#     log_pipeline_metrics(\"orders_pipeline\", \"FAILED\", 0, str(e))\n#     raise\n\nprint(\"\u2705 Monitoring pattern\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## 7. Key Takeaways\n\n### Production-Ready Checklist:\n\n- \u2705 **Idempotent**: Can safely re-run without duplicating data\n- \u2705 **Incremental**: Only processes new/changed data\n- \u2705 **Validated**: Data quality checks at each layer\n- \u2705 **Monitored**: Logs metrics for observability\n- \u2705 **Resilient**: Handles errors gracefully with DLQ\n- \u2705 **Performant**: Optimized with partitioning and Z-ordering\n- \u2705 **Documented**: Clear lineage and metadata\n- \u2705 **Tested**: Unit and integration tests\n\n### Common Anti-Patterns to Avoid:\n\n- \u274c Full table scans on every run\n- \u274c No data quality checks\n- \u274c Silent failures (no error handling)\n- \u274c Hardcoded values instead of parameters\n- \u274c No monitoring or alerting\n- \u274c Overwriting data without history\n- \u274c Missing indexes on large tables\n\n### Resources:\n\n- [Delta Lake Best Practices](https://docs.delta.io/latest/best-practices.html)\n- [Databricks Performance Tuning](https://docs.databricks.com/optimizations/index.html)\n- [Data Quality on Databricks](https://www.databricks.com/blog/2022/01/19/data-quality-at-scale-with-databricks.html)\n\n---\n\n**Continue learning and building robust data pipelines! \ud83d\ude80**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}