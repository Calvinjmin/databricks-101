{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Databricks Data Engineering 101: Medallion Architecture\n",
        "\n",
        "**Build Production-Ready Data Pipelines with Bronze, Silver & Gold Layers**\n",
        "\n",
        "Welcome to the core of data engineering on Databricks! In this notebook, you'll learn to:\n",
        "\n",
        "- ü•â **Bronze Layer**: Ingest raw data with complete history\n",
        "- ü•à **Silver Layer**: Clean, validate, and standardize data\n",
        "- ü•á **Gold Layer**: Create business-ready analytics tables\n",
        "\n",
        "## What is Medallion Architecture?\n",
        "\n",
        "The Medallion Architecture organizes data into three progressive layers:\n",
        "\n",
        "```\n",
        "Raw Data ‚Üí [Bronze] ‚Üí [Silver] ‚Üí [Gold] ‚Üí Business Insights\n",
        "           Raw         Clean      Analytics\n",
        "```\n",
        "\n",
        "### Benefits:\n",
        "- **Incremental Refinement**: Each layer adds value\n",
        "- **Data Quality**: Progressive validation and cleansing\n",
        "- **Performance**: Optimized for different use cases\n",
        "- **Flexibility**: Easy to add new sources or metrics\n",
        "\n",
        "### Real-World Use Cases:\n",
        "- **Finance**: Transaction processing and fraud detection\n",
        "- **E-commerce**: Customer analytics and product performance\n",
        "- **Healthcare**: Patient records and clinical analytics\n",
        "- **IoT**: Sensor data processing and anomaly detection\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SETUP\n",
        "\n",
        "Just run the next couple of cells for setup!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Widgets - Customize your setup\n",
        "\n",
        "dbutils.widgets.text(\"catalog\", \"demo\", \"Catalog\")\n",
        "dbutils.widgets.text(\"bronze_db\", \"bronze\", \"Bronze DB\")\n",
        "dbutils.widgets.text(\"silver_db\", \"silver\", \"Silver DB\")\n",
        "dbutils.widgets.text(\"gold_db\", \"gold\", \"Gold DB\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "bronze_db = dbutils.widgets.get(\"bronze_db\")\n",
        "silver_db = dbutils.widgets.get(\"silver_db\")\n",
        "gold_db = dbutils.widgets.get(\"gold_db\")\n",
        "\n",
        "# Define path for data storage in Volume\n",
        "path = f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files/\"\n",
        "\n",
        "print(f\"Catalog: {catalog}\")\n",
        "print(f\"Bronze DB: {bronze_db}\")\n",
        "print(f\"Silver DB: {silver_db}\")\n",
        "print(f\"Gold DB: {gold_db}\")\n",
        "print(f\"Path: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Unity Catalog, Schemas, and Volume\n",
        "\n",
        "# Create catalog\n",
        "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
        "\n",
        "# Use catalog\n",
        "spark.sql(f\"USE CATALOG {catalog}\")\n",
        "\n",
        "# Create databases/schemas\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {gold_db}\")\n",
        "\n",
        "# Create volume for data storage\n",
        "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {bronze_db}.ecommerce\")\n",
        "\n",
        "print(f\"‚úÖ Created catalog: {catalog}\")\n",
        "print(f\"‚úÖ Created schemas: {bronze_db}, {silver_db}, {gold_db}\")\n",
        "print(f\"‚úÖ Created volume: {bronze_db}.ecommerce\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create volume folders for data organization\n",
        "\n",
        "try:\n",
        "    dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files/customers\")\n",
        "    dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files/products\")\n",
        "    dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files/orders\")\n",
        "    dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files/order_items\")\n",
        "    dbutils.fs.mkdirs(f\"/Volumes/{catalog}/{bronze_db}/ecommerce/downloads\")\n",
        "    print(\"‚úÖ Created volume folder structure\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Volume folders may already exist: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì• Load Sample Data\n",
        "\n",
        "Copy CSV files to Unity Catalog Volume:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"BRONZE LAYER: DATA INGESTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Relative Imports\n",
        "notebook_dir = os.getcwd()\n",
        "repo_data_dir = os.path.abspath(os.path.join(notebook_dir, \"../data\"))\n",
        "\n",
        "if not os.path.isdir(repo_data_dir):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find ../data folder relative to the notebook at {notebook_dir}. \"\n",
        "        \"Please ensure the data directory exists.\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"\\nüì¶ Using relative data path: {repo_data_dir}\")\n",
        "\n",
        "# Target directory: Unity Catalog Volume \n",
        "volume_base = f\"/Volumes/{catalog}/{bronze_db}/ecommerce/files\"\n",
        "downloads_dir = f\"/Volumes/{catalog}/{bronze_db}/ecommerce/downloads\"\n",
        "\n",
        "print(f\"\\nüìã Creating Bronze tables with PySpark...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tables = {\n",
        "    \"customers\": f\"{catalog}.{bronze_db}.customers\",\n",
        "    \"products\": f\"{catalog}.{bronze_db}.products\",\n",
        "    \"orders\": f\"{catalog}.{bronze_db}.orders\",\n",
        "    \"order_items\": f\"{catalog}.{bronze_db}.order_items\"\n",
        "}\n",
        "\n",
        "for csv_name in tables:\n",
        "    source_path = os.path.join(repo_data_dir, f\"{csv_name}.csv\")\n",
        "    target_dir = f\"{volume_base}/{csv_name}\"\n",
        "    target_path = f\"{target_dir}/{csv_name}.csv\"\n",
        "\n",
        "    # Only attempt to copy if file doesn't already exist in volume\n",
        "    if not os.path.exists(target_path):\n",
        "        try:\n",
        "            shutil.copyfile(source_path, target_path)\n",
        "            print(f\"   ‚úÖ Copied {csv_name}.csv to {target_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Could not copy {csv_name}.csv to {target_dir}: {e}\")\n",
        "    else:\n",
        "        print(f\"   ‚è≠Ô∏è {csv_name}.csv already exists in volume, skipping copy.\")\n",
        "\n",
        "# Now, read from the Unity Catalog volume paths and create Bronze tables\n",
        "for csv_name, table_name in tables.items():\n",
        "    print(f\"\\n‚è≥ Processing {csv_name}.csv...\")\n",
        "\n",
        "    volume_csv_path = f\"{volume_base}/{csv_name}/{csv_name}.csv\"\n",
        "    # No \"file:\" URI prefix (use UC/DBFS path)\n",
        "    df = spark.read.csv(volume_csv_path, header=True, inferSchema=True)\n",
        "    df = df.withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
        "           .withColumn(\"_source_file\", lit(f\"{csv_name}.csv\"))\n",
        "    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
        "    count = spark.table(table_name).count()\n",
        "    print(f\"   ‚úÖ {table_name}: {count:,} records\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ BRONZE LAYER COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nAll CSV data loaded into Unity Catalog: {catalog}\")\n",
        "print(f\"Tables created in {catalog}.{bronze_db} schema\")\n",
        "print(\"You can now proceed to the Silver layer below.\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ü•â Bronze Layer: Verify & Explore\n",
        "\n",
        "The Bronze tables have been created above! Let's verify and explore the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Bronze Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Show all Bronze tables in Unity Catalog\n",
        "display(spark.sql(f\"SHOW TABLES IN {catalog}.{bronze_db}\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Check record counts (Unity Catalog 3-level naming)\n",
        "from pyspark.sql.functions import lit, count\n",
        "\n",
        "customers_df = spark.table(f\"{catalog}.{bronze_db}.customers\").select(lit('customers').alias('table_name'), count('*').alias('record_count'))\n",
        "products_df = spark.table(f\"{catalog}.{bronze_db}.products\").select(lit('products').alias('table_name'), count('*').alias('record_count'))\n",
        "orders_df = spark.table(f\"{catalog}.{bronze_db}.orders\").select(lit('orders').alias('table_name'), count('*').alias('record_count'))\n",
        "order_items_df = spark.table(f\"{catalog}.{bronze_db}.order_items\").select(lit('order_items').alias('table_name'), count('*').alias('record_count'))\n",
        "\n",
        "result_df = customers_df.union(products_df).union(orders_df).union(order_items_df)\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Preview customers data\n",
        "display(spark.table(f\"{catalog}.{bronze_db}.customers\").limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview Other Tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Preview products\n",
        "display(spark.table(f\"{catalog}.{bronze_db}.products\").limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Quality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Check for any null key columns (should be none)\n",
        "from pyspark.sql.functions import lit, sum, when, col\n",
        "\n",
        "customers_nulls = spark.table(f\"{catalog}.{bronze_db}.customers\").select(\n",
        "    lit('customers').alias('table_name'),\n",
        "    sum(when(col('customer_id').isNull(), 1).otherwise(0)).alias('null_ids')\n",
        ")\n",
        "\n",
        "products_nulls = spark.table(f\"{catalog}.{bronze_db}.products\").select(\n",
        "    lit('products').alias('table_name'),\n",
        "    sum(when(col('product_id').isNull(), 1).otherwise(0)).alias('null_ids')\n",
        ")\n",
        "\n",
        "orders_nulls = spark.table(f\"{catalog}.{bronze_db}.orders\").select(\n",
        "    lit('orders').alias('table_name'),\n",
        "    sum(when(col('order_id').isNull(), 1).otherwise(0)).alias('null_ids')\n",
        ")\n",
        "\n",
        "order_items_nulls = spark.table(f\"{catalog}.{bronze_db}.order_items\").select(\n",
        "    lit('order_items').alias('table_name'),\n",
        "    sum(when(col('line_item_id').isNull(), 1).otherwise(0)).alias('null_ids')\n",
        ")\n",
        "\n",
        "result_df = customers_nulls.union(products_nulls).union(orders_nulls).union(order_items_nulls)\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Bronze Layer Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"BRONZE LAYER SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCatalog: {catalog}\")\n",
        "print(f\"\\nCustomers:    {spark.table(f'{catalog}.bronze.customers').count():>10,} records\")\n",
        "print(f\"Products:     {spark.table(f'{catalog}.bronze.products').count():>10,} records\")\n",
        "print(f\"Orders:       {spark.table(f'{catalog}.bronze.orders').count():>10,} records\")\n",
        "print(f\"Order Items:  {spark.table(f'{catalog}.bronze.order_items').count():>10,} records\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ Bronze Layer Complete - Raw data in Unity Catalog\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nKey Features:\")\n",
        "print(\"  ‚Ä¢ Unity Catalog 3-level namespacing (catalog.schema.table)\")\n",
        "print(\"  ‚Ä¢ All data stored in Delta Lake format (ACID compliant)\")\n",
        "print(\"  ‚Ä¢ Metadata columns added (_ingestion_timestamp, _source_file)\")\n",
        "print(\"  ‚Ä¢ Ready for cleansing in Silver layer\")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ü•à Silver Layer: Data Cleansing & Validation\n",
        "\n",
        "## Goals:\n",
        "- **Clean**: Remove nulls, fix data types, standardize formats\n",
        "- **Validate**: Apply business rules and constraints\n",
        "- **Deduplicate**: Keep only unique, valid records\n",
        "- **Enrich**: Add derived columns for downstream use\n",
        "\n",
        "## Key Patterns:\n",
        "- Data quality checks\n",
        "- Deduplication using window functions\n",
        "- Type casting and formatting\n",
        "- Business rule validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silver: Customers (Cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Silver customers with data quality rules\n",
        "from pyspark.sql.functions import col, initcap, trim, lower, upper, current_timestamp\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{silver_db}.customers\")\n",
        "\n",
        "# Read from Bronze and apply transformations\n",
        "customers_df = spark.table(f\"{catalog}.{bronze_db}.customers\") \\\n",
        "    .select(\n",
        "        col(\"customer_id\"),\n",
        "        initcap(trim(col(\"first_name\"))).alias(\"first_name\"),\n",
        "        initcap(trim(col(\"last_name\"))).alias(\"last_name\"),\n",
        "        lower(trim(col(\"email\"))).alias(\"email\"),\n",
        "        col(\"phone\"),\n",
        "        col(\"address\"),\n",
        "        col(\"city\"),\n",
        "        upper(col(\"state\")).alias(\"state\"),\n",
        "        col(\"zip_code\"),\n",
        "        col(\"country\"),\n",
        "        col(\"registration_date\"),\n",
        "        col(\"customer_segment\"),\n",
        "        current_timestamp().alias(\"updated_at\")\n",
        "    ) \\\n",
        "    .filter(\n",
        "        (col(\"customer_id\").isNotNull()) &\n",
        "        (col(\"email\").isNotNull()) &\n",
        "        (col(\"email\").like(\"%@%\")) &\n",
        "        (col(\"registration_date\").isNotNull())\n",
        "    )\n",
        "\n",
        "# Write to Silver table\n",
        "customers_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{silver_db}.customers\")\n",
        "\n",
        "# Show count\n",
        "cleaned_count = spark.table(f\"{catalog}.{silver_db}.customers\").count()\n",
        "print(f\"cleaned_count: {cleaned_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Compare Bronze vs Silver\n",
        "from pyspark.sql.functions import lit, count\n",
        "\n",
        "bronze_count = spark.table(f\"{catalog}.{bronze_db}.customers\").select(\n",
        "    lit('Bronze').alias('layer'),\n",
        "    count('*').alias('record_count')\n",
        ")\n",
        "\n",
        "silver_count = spark.table(f\"{catalog}.{silver_db}.customers\").select(\n",
        "    lit('Silver').alias('layer'),\n",
        "    count('*').alias('record_count')\n",
        ")\n",
        "\n",
        "result_df = bronze_count.union(silver_count)\n",
        "display(result_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silver: Products (Cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Silver products with data quality rules\n",
        "from pyspark.sql.functions import col, trim, when, current_timestamp\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{silver_db}.products\")\n",
        "\n",
        "# Read from Bronze and apply transformations\n",
        "products_df = spark.table(f\"{catalog}.{bronze_db}.products\") \\\n",
        "    .select(\n",
        "        col(\"product_id\"),\n",
        "        trim(col(\"product_name\")).alias(\"product_name\"),\n",
        "        col(\"category\"),\n",
        "        col(\"brand\"),\n",
        "        col(\"price\"),\n",
        "        col(\"stock_quantity\"),\n",
        "        col(\"is_active\"),\n",
        "        when(col(\"price\") < 50, \"Budget\")\n",
        "            .when(col(\"price\") < 200, \"Mid-Range\")\n",
        "            .otherwise(\"Premium\").alias(\"price_tier\"),\n",
        "        current_timestamp().alias(\"updated_at\")\n",
        "    ) \\\n",
        "    .filter(\n",
        "        (col(\"product_id\").isNotNull()) &\n",
        "        (col(\"product_name\").isNotNull()) &\n",
        "        (col(\"price\") > 0) &\n",
        "        (col(\"price\") < 10000)\n",
        "    )\n",
        "\n",
        "# Write to Silver table\n",
        "products_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{silver_db}.products\")\n",
        "\n",
        "# Show count\n",
        "cleaned_count = spark.table(f\"{catalog}.{silver_db}.products\").count()\n",
        "print(f\"cleaned_count: {cleaned_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silver: Orders (Cleaned & Enriched)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Silver orders with data quality rules and enrichment\n",
        "from pyspark.sql.functions import col, when, datediff, year, month, dayofweek, current_timestamp, to_date\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{silver_db}.orders\")\n",
        "\n",
        "# Read from Bronze and apply transformations\n",
        "orders_df = spark.table(f\"{catalog}.{bronze_db}.orders\") \\\n",
        "    .select(\n",
        "        col(\"order_id\"),\n",
        "        col(\"customer_id\"),\n",
        "        col(\"order_date\"),\n",
        "        col(\"status\"),\n",
        "        col(\"payment_method\"),\n",
        "        col(\"shipped_date\"),\n",
        "        col(\"delivered_date\"),\n",
        "        col(\"discount_percent\"),\n",
        "        when(col(\"delivered_date\").isNotNull(), \n",
        "             datediff(col(\"delivered_date\"), to_date(col(\"order_date\"))))\n",
        "            .otherwise(None).alias(\"days_to_deliver\"),\n",
        "        year(col(\"order_date\")).alias(\"order_year\"),\n",
        "        month(col(\"order_date\")).alias(\"order_month\"),\n",
        "        dayofweek(col(\"order_date\")).alias(\"order_day_of_week\"),\n",
        "        current_timestamp().alias(\"updated_at\")\n",
        "    ) \\\n",
        "    .filter(\n",
        "        (col(\"order_id\").isNotNull()) &\n",
        "        (col(\"customer_id\").isNotNull()) &\n",
        "        (col(\"order_date\").isNotNull()) &\n",
        "        (col(\"status\").isin('Completed', 'Shipped', 'Processing', 'Cancelled'))\n",
        "    )\n",
        "\n",
        "# Write to Silver table\n",
        "orders_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{silver_db}.orders\")\n",
        "\n",
        "# Show count\n",
        "cleaned_count = spark.table(f\"{catalog}.{silver_db}.orders\").count()\n",
        "print(f\"cleaned_count: {cleaned_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Silver: Order Items (Cleaned with Calculations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Silver order_items with calculations\n",
        "from pyspark.sql.functions import col, current_timestamp\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{silver_db}.order_items\")\n",
        "\n",
        "# Read from Bronze and apply transformations\n",
        "order_items_df = spark.table(f\"{catalog}.{bronze_db}.order_items\") \\\n",
        "    .select(\n",
        "        col(\"line_item_id\"),\n",
        "        col(\"order_id\"),\n",
        "        col(\"product_id\"),\n",
        "        col(\"quantity\"),\n",
        "        col(\"unit_price\"),\n",
        "        (col(\"quantity\") * col(\"unit_price\")).alias(\"line_total\"),\n",
        "        current_timestamp().alias(\"updated_at\")\n",
        "    ) \\\n",
        "    .filter(\n",
        "        (col(\"line_item_id\").isNotNull()) &\n",
        "        (col(\"order_id\").isNotNull()) &\n",
        "        (col(\"product_id\").isNotNull()) &\n",
        "        (col(\"quantity\") > 0) &\n",
        "        (col(\"quantity\") <= 100) &\n",
        "        (col(\"unit_price\") > 0) &\n",
        "        (col(\"unit_price\") < 10000)\n",
        "    )\n",
        "\n",
        "# Write to Silver table\n",
        "order_items_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{silver_db}.order_items\")\n",
        "\n",
        "# Show count\n",
        "cleaned_count = spark.table(f\"{catalog}.{silver_db}.order_items\").count()\n",
        "print(f\"cleaned_count: {cleaned_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Silver Layer Complete!\n",
        "\n",
        "Summary of our cleansed data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Bronze vs Silver record counts\n",
        "print(\"=\"*70)\n",
        "print(\"BRONZE ‚Üí SILVER DATA QUALITY REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tables = ['customers', 'products', 'orders', 'order_items']\n",
        "for table in tables:\n",
        "    bronze_count = spark.table(f'{catalog}.{bronze_db}.{table}').count()\n",
        "    silver_count = spark.table(f'{catalog}.{silver_db}.{table}').count()\n",
        "    rejected = bronze_count - silver_count\n",
        "    rejection_rate = (rejected / bronze_count * 100) if bronze_count > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{table.upper()}:\")\n",
        "    print(f\"  Bronze: {bronze_count:>10,}\")\n",
        "    print(f\"  Silver: {silver_count:>10,}\")\n",
        "    print(f\"  Rejected: {rejected:>8,} ({rejection_rate:.2f}%)\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# ü•á Gold Layer: Business Analytics\n",
        "\n",
        "## Goals:\n",
        "- Create **business-ready** tables optimized for reporting\n",
        "- Pre-calculate **metrics and KPIs**\n",
        "- Denormalize data for **fast queries**\n",
        "- Support **dashboards and analytics**\n",
        "\n",
        "## Patterns:\n",
        "- Aggregations and rollups\n",
        "- Star schema / dimensional modeling\n",
        "- Pre-calculated metrics\n",
        "- Optimized for BI tools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gold: Customer Analytics\n",
        "\n",
        "Calculate customer lifetime value and segmentation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Gold customer_analytics table\n",
        "from pyspark.sql.functions import col, count, sum, avg, max, min, datediff, when, coalesce, lit, current_timestamp, countDistinct\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{gold_db}.customer_analytics\")\n",
        "\n",
        "# Create order_totals CTE\n",
        "order_totals = spark.table(f\"{catalog}.{silver_db}.orders\").alias(\"o\") \\\n",
        "    .join(spark.table(f\"{catalog}.{silver_db}.order_items\").alias(\"oi\"), \n",
        "          col(\"o.order_id\") == col(\"oi.order_id\"), \"inner\") \\\n",
        "    .filter(col(\"o.status\") != \"Cancelled\") \\\n",
        "    .groupBy(\"o.customer_id\", \"o.order_id\") \\\n",
        "    .agg(sum(\"oi.line_total\").alias(\"order_total\"))\n",
        "\n",
        "# Main query\n",
        "customers = spark.table(f\"{catalog}.{silver_db}.customers\").alias(\"c\")\n",
        "orders = spark.table(f\"{catalog}.{silver_db}.orders\").alias(\"o\")\n",
        "\n",
        "customer_analytics_df = customers \\\n",
        "    .join(orders, \n",
        "          (col(\"c.customer_id\") == col(\"o.customer_id\")) & (col(\"o.status\") != \"Cancelled\"), \n",
        "          \"left\") \\\n",
        "    .join(order_totals.alias(\"ot\"), \n",
        "          col(\"o.order_id\") == col(\"ot.order_id\"), \n",
        "          \"left\") \\\n",
        "    .groupBy(\n",
        "        \"c.customer_id\", \"c.first_name\", \"c.last_name\", \"c.email\",\n",
        "        \"c.city\", \"c.state\", \"c.customer_segment\", \"c.registration_date\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        countDistinct(\"o.order_id\").alias(\"total_orders\"),\n",
        "        coalesce(sum(\"ot.order_total\"), lit(0)).alias(\"lifetime_value\"),\n",
        "        coalesce(avg(\"ot.order_total\"), lit(0)).alias(\"avg_order_value\"),\n",
        "        max(\"o.order_date\").alias(\"last_order_date\"),\n",
        "        min(\"o.order_date\").alias(\"first_order_date\"),\n",
        "        datediff(max(\"o.order_date\"), min(\"o.order_date\")).alias(\"customer_tenure_days\")\n",
        "    ) \\\n",
        "    .withColumn(\"orders_per_month\",\n",
        "        when(col(\"customer_tenure_days\") > 0,\n",
        "             col(\"total_orders\") * 30.0 / col(\"customer_tenure_days\"))\n",
        "        .otherwise(0)\n",
        "    ) \\\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        "\n",
        "# Write to Gold table\n",
        "customer_analytics_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{gold_db}.customer_analytics\")\n",
        "\n",
        "# Show count\n",
        "customer_count = spark.table(f\"{catalog}.{gold_db}.customer_analytics\").count()\n",
        "print(f\"customer_count: {customer_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Top 10 customers by lifetime value\n",
        "from pyspark.sql.functions import col, concat_ws, round\n",
        "\n",
        "top_customers_df = spark.table(f\"{catalog}.{gold_db}.customer_analytics\") \\\n",
        "    .select(\n",
        "        col(\"customer_id\"),\n",
        "        concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"customer_name\"),\n",
        "        col(\"email\"),\n",
        "        col(\"total_orders\"),\n",
        "        round(col(\"lifetime_value\"), 2).alias(\"lifetime_value\"),\n",
        "        round(col(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
        "        col(\"customer_segment\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"lifetime_value\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "display(top_customers_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gold: Product Performance\n",
        "\n",
        "Analyze product sales and revenue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Gold product_performance table\n",
        "from pyspark.sql.functions import col, countDistinct, sum, avg, current_timestamp, rank\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{gold_db}.product_performance\")\n",
        "\n",
        "# Read tables\n",
        "products = spark.table(f\"{catalog}.{silver_db}.products\").alias(\"p\")\n",
        "order_items = spark.table(f\"{catalog}.{silver_db}.order_items\").alias(\"oi\")\n",
        "orders = spark.table(f\"{catalog}.{silver_db}.orders\").alias(\"o\")\n",
        "\n",
        "# Join and aggregate\n",
        "product_metrics = products \\\n",
        "    .join(order_items, col(\"p.product_id\") == col(\"oi.product_id\"), \"left\") \\\n",
        "    .join(orders, (col(\"oi.order_id\") == col(\"o.order_id\")) & (col(\"o.status\") != \"Cancelled\"), \"left\") \\\n",
        "    .groupBy(\n",
        "        \"p.product_id\", \"p.product_name\", \"p.category\", \"p.brand\", \"p.price\", \"p.price_tier\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        countDistinct(\"oi.order_id\").alias(\"orders_containing_product\"),\n",
        "        sum(\"oi.quantity\").alias(\"total_quantity_sold\"),\n",
        "        sum(\"oi.line_total\").alias(\"total_revenue\"),\n",
        "        avg(\"oi.unit_price\").alias(\"avg_selling_price\")\n",
        "    )\n",
        "\n",
        "# Add window function for ranking\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "product_performance_df = product_metrics \\\n",
        "    .withColumn(\"revenue_rank_in_category\", rank().over(window_spec)) \\\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        "\n",
        "# Write to Gold table\n",
        "product_performance_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{gold_db}.product_performance\")\n",
        "\n",
        "# Show count\n",
        "product_count = spark.table(f\"{catalog}.{gold_db}.product_performance\").count()\n",
        "print(f\"product_count: {product_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Top 10 products by revenue\n",
        "from pyspark.sql.functions import col, round\n",
        "\n",
        "top_products_df = spark.table(f\"{catalog}.{gold_db}.product_performance\") \\\n",
        "    .select(\n",
        "        col(\"product_name\"),\n",
        "        col(\"category\"),\n",
        "        col(\"brand\"),\n",
        "        round(col(\"total_revenue\"), 2).alias(\"total_revenue\"),\n",
        "        col(\"total_quantity_sold\"),\n",
        "        col(\"orders_containing_product\"),\n",
        "        col(\"revenue_rank_in_category\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"total_revenue\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "display(top_products_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gold: Monthly Revenue Trends\n",
        "\n",
        "Time-series analysis for business reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Gold monthly_revenue table\n",
        "from pyspark.sql.functions import col, sum, count, avg, round, current_timestamp, countDistinct, date_trunc, lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{gold_db}.monthly_revenue\")\n",
        "\n",
        "# Create order_revenues CTE\n",
        "orders = spark.table(f\"{catalog}.{silver_db}.orders\").alias(\"o\")\n",
        "order_items = spark.table(f\"{catalog}.{silver_db}.order_items\").alias(\"oi\")\n",
        "\n",
        "order_revenues = orders \\\n",
        "    .join(order_items, col(\"o.order_id\") == col(\"oi.order_id\"), \"inner\") \\\n",
        "    .filter(col(\"o.status\") != \"Cancelled\") \\\n",
        "    .groupBy(\"o.order_id\", \"o.order_date\", \"o.order_year\", \"o.order_month\", \"o.status\", \"o.discount_percent\") \\\n",
        "    .agg(\n",
        "        sum(\"oi.line_total\").alias(\"order_total\"),\n",
        "        sum(col(\"oi.line_total\") * col(\"o.discount_percent\") / 100).alias(\"discount_amount\")\n",
        "    )\n",
        "\n",
        "# Aggregate by month\n",
        "monthly_agg = order_revenues \\\n",
        "    .withColumn(\"month_start_date\", date_trunc(\"month\", col(\"order_date\"))) \\\n",
        "    .groupBy(\"order_year\", \"order_month\", \"month_start_date\") \\\n",
        "    .agg(\n",
        "        countDistinct(\"order_id\").alias(\"total_orders\"),\n",
        "        sum(\"order_total\").alias(\"gross_revenue\"),\n",
        "        sum(\"discount_amount\").alias(\"total_discounts\"),\n",
        "        sum(col(\"order_total\") - col(\"discount_amount\")).alias(\"net_revenue\"),\n",
        "        avg(\"order_total\").alias(\"avg_order_value\")\n",
        "    )\n",
        "\n",
        "# Add window functions for month-over-month growth\n",
        "window_spec = Window.orderBy(\"order_year\", \"order_month\")\n",
        "\n",
        "monthly_revenue_df = monthly_agg \\\n",
        "    .withColumn(\"prev_month_revenue\", lag(\"gross_revenue\").over(window_spec)) \\\n",
        "    .withColumn(\"mom_growth_percent\",\n",
        "        round(\n",
        "            (col(\"gross_revenue\") - col(\"prev_month_revenue\")) / col(\"prev_month_revenue\") * 100,\n",
        "            2\n",
        "        )\n",
        "    ) \\\n",
        "    .withColumn(\"calculated_at\", current_timestamp()) \\\n",
        "    .orderBy(\"order_year\", \"order_month\")\n",
        "\n",
        "# Write to Gold table\n",
        "monthly_revenue_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{gold_db}.monthly_revenue\")\n",
        "\n",
        "# Show count\n",
        "month_count = spark.table(f\"{catalog}.{gold_db}.monthly_revenue\").count()\n",
        "print(f\"month_count: {month_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# View monthly revenue trends\n",
        "from pyspark.sql.functions import col, to_date, round\n",
        "\n",
        "monthly_trends_df = spark.table(f\"{catalog}.{gold_db}.monthly_revenue\") \\\n",
        "    .select(\n",
        "        to_date(col(\"month_start_date\")).alias(\"month\"),\n",
        "        col(\"total_orders\"),\n",
        "        round(col(\"gross_revenue\"), 2).alias(\"gross_revenue\"),\n",
        "        round(col(\"net_revenue\"), 2).alias(\"net_revenue\"),\n",
        "        round(col(\"avg_order_value\"), 2).alias(\"avg_order_value\"),\n",
        "        col(\"mom_growth_percent\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"month\").desc()) \\\n",
        "    .limit(12)\n",
        "\n",
        "display(monthly_trends_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gold: Category Performance\n",
        "\n",
        "Category-level analytics for merchandising decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Create Gold category_performance table\n",
        "from pyspark.sql.functions import col, countDistinct, sum, avg, min, max, current_timestamp\n",
        "\n",
        "# Drop table if exists\n",
        "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{gold_db}.category_performance\")\n",
        "\n",
        "# Read tables\n",
        "products = spark.table(f\"{catalog}.{silver_db}.products\").alias(\"p\")\n",
        "order_items = spark.table(f\"{catalog}.{silver_db}.order_items\").alias(\"oi\")\n",
        "orders = spark.table(f\"{catalog}.{silver_db}.orders\").alias(\"o\")\n",
        "\n",
        "# Join and aggregate\n",
        "category_performance_df = products \\\n",
        "    .join(order_items, col(\"p.product_id\") == col(\"oi.product_id\"), \"left\") \\\n",
        "    .join(orders, (col(\"oi.order_id\") == col(\"o.order_id\")) & (col(\"o.status\") != \"Cancelled\"), \"left\") \\\n",
        "    .groupBy(\"p.category\") \\\n",
        "    .agg(\n",
        "        countDistinct(\"p.product_id\").alias(\"total_products\"),\n",
        "        countDistinct(\"oi.order_id\").alias(\"total_orders\"),\n",
        "        sum(\"oi.quantity\").alias(\"total_units_sold\"),\n",
        "        sum(\"oi.line_total\").alias(\"total_revenue\"),\n",
        "        avg(\"oi.line_total\").alias(\"avg_transaction_value\"),\n",
        "        min(\"p.price\").alias(\"min_price\"),\n",
        "        max(\"p.price\").alias(\"max_price\"),\n",
        "        avg(\"p.price\").alias(\"avg_price\")\n",
        "    ) \\\n",
        "    .withColumn(\"calculated_at\", current_timestamp())\n",
        "\n",
        "# Write to Gold table\n",
        "category_performance_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{gold_db}.category_performance\")\n",
        "\n",
        "# Display results ordered by revenue\n",
        "display(spark.table(f\"{catalog}.{gold_db}.category_performance\").orderBy(col(\"total_revenue\").desc()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ Gold Layer Complete!\n",
        "\n",
        "Summary of all Gold tables:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Show all Gold tables\n",
        "display(spark.sql(f\"SHOW TABLES IN {catalog}.{gold_db}\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gold layer summary\n",
        "print(\"=\"*70)\n",
        "print(\"GOLD LAYER SUMMARY - BUSINESS ANALYTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCustomer Analytics:  {spark.table(f'{catalog}.{gold_db}.customer_analytics').count():>10,} customers\")\n",
        "print(f\"Product Performance: {spark.table(f'{catalog}.{gold_db}.product_performance').count():>10,} products\")\n",
        "print(f\"Monthly Revenue:     {spark.table(f'{catalog}.{gold_db}.monthly_revenue').count():>10,} months\")\n",
        "print(f\"Category Performance:{spark.table(f'{catalog}.{gold_db}.category_performance').count():>10,} categories\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéì Advanced Concepts\n",
        "\n",
        "## Delta Lake Features You've Used\n",
        "\n",
        "Throughout this notebook, you've leveraged powerful Delta Lake capabilities:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Time Travel\n",
        "\n",
        "Query previous versions of your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# View table history\n",
        "display(spark.sql(f\"DESCRIBE HISTORY {catalog}.{silver_db}.customers LIMIT 5\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Query a previous version (if you've updated the table)\n",
        "# display(spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(f\"{catalog}.{silver_db}.customers\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Table Statistics & Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# View detailed table information\n",
        "display(spark.sql(f\"DESCRIBE EXTENDED {catalog}.{gold_db}.customer_analytics\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Optimize tables for better query performance\n",
        "# spark.sql(f\"OPTIMIZE {catalog}.{gold_db}.customer_analytics\")\n",
        "# spark.sql(f\"OPTIMIZE {catalog}.{gold_db}.product_performance\")\n",
        "# spark.sql(f\"OPTIMIZE {catalog}.{gold_db}.monthly_revenue\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# üéâ Congratulations!\n",
        "\n",
        "You've successfully built a complete **Medallion Architecture** on Databricks!\n",
        "\n",
        "## What You Accomplished:\n",
        "\n",
        "### ‚úÖ Bronze Layer\n",
        "- Ingested raw CSV data into Delta tables\n",
        "- Preserved complete data history\n",
        "- Used idempotent `COPY INTO` pattern\n",
        "\n",
        "### ‚úÖ Silver Layer\n",
        "- Cleaned and validated data\n",
        "- Applied business rules\n",
        "- Added derived columns\n",
        "- Standardized formats\n",
        "\n",
        "### ‚úÖ Gold Layer\n",
        "- Created business-ready analytics tables\n",
        "- Pre-calculated KPIs and metrics\n",
        "- Built customer lifetime value analysis\n",
        "- Analyzed product and category performance\n",
        "- Created time-series revenue trends\n",
        "\n",
        "## Key Concepts Mastered:\n",
        "\n",
        "- üì¶ **Delta Lake**: ACID transactions, time travel, schema evolution\n",
        "- üèóÔ∏è **Medallion Architecture**: Progressive data refinement\n",
        "- üîÑ **ETL Patterns**: Incremental loading, data quality, transformations\n",
        "- üìä **Analytics Engineering**: Business metrics, aggregations, rankings\n",
        "- üöÄ **Performance**: Optimizations, partitioning strategies\n",
        "\n",
        "## Next Steps:\n",
        "\n",
        "1. **Explore Further**: Try modifying queries to answer your own business questions\n",
        "2. **Add Complexity**: Implement slowly changing dimensions (SCD Type 2)\n",
        "3. **Automation**: Learn about Databricks Workflows to schedule these pipelines\n",
        "4. **Streaming**: Explore Structured Streaming for real-time data\n",
        "5. **ML Integration**: Build machine learning models on your clean data\n",
        "6. **Check Best Practices**: Review notebook 03 for advanced patterns\n",
        "\n",
        "---\n",
        "\n",
        "## Sample Business Questions You Can Answer:\n",
        "\n",
        "```sql\n",
        "-- Who are the most valuable customers?\n",
        "SELECT * FROM {catalog}.{gold_db}.customer_analytics \n",
        "ORDER BY lifetime_value DESC LIMIT 10;\n",
        "\n",
        "-- What products drive the most revenue?\n",
        "SELECT * FROM {catalog}.{gold_db}.product_performance \n",
        "ORDER BY total_revenue DESC LIMIT 10;\n",
        "\n",
        "-- How is revenue trending?\n",
        "SELECT * FROM {catalog}.{gold_db}.monthly_revenue \n",
        "ORDER BY order_year DESC, order_month DESC;\n",
        "\n",
        "-- Which categories perform best?\n",
        "SELECT * FROM {catalog}.{gold_db}.category_performance \n",
        "ORDER BY total_revenue DESC;\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**You're now ready to build production data pipelines on Databricks! üöÄ**\n",
        "\n",
        "Questions? Check out:\n",
        "- [Databricks Documentation](https://docs.databricks.com/)\n",
        "- [Delta Lake Guide](https://docs.delta.io/)\n",
        "- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
